{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\studwilksa2535\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\tensor_shape_pb2.py:18: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "C:\\Users\\studwilksa2535\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\tensor_shape_pb2.py:36: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _descriptor.FieldDescriptor(\n",
      "C:\\Users\\studwilksa2535\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\tensor_shape_pb2.py:29: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _TENSORSHAPEPROTO_DIM = _descriptor.Descriptor(\n",
      "C:\\Users\\studwilksa2535\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:19: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "C:\\Users\\studwilksa2535\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:33: DeprecationWarning: Call to deprecated create function EnumValueDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _descriptor.EnumValueDescriptor(\n",
      "C:\\Users\\studwilksa2535\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:27: DeprecationWarning: Call to deprecated create function EnumDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _DATATYPE = _descriptor.EnumDescriptor(\n",
      "C:\\Users\\studwilksa2535\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\resource_handle_pb2.py:20: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "C:\\Users\\studwilksa2535\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\resource_handle_pb2.py:39: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _descriptor.FieldDescriptor(\n",
      "C:\\Users\\studwilksa2535\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\resource_handle_pb2.py:32: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _RESOURCEHANDLEPROTO_DTYPEANDSHAPE = _descriptor.Descriptor(\n",
      "C:\\Users\\studwilksa2535\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:21: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "C:\\Users\\studwilksa2535\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:40: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _descriptor.FieldDescriptor(\n",
      "C:\\Users\\studwilksa2535\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:33: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _TENSORPROTO = _descriptor.Descriptor(\n",
      "C:\\Users\\studwilksa2535\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\attr_value_pb2.py:21: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "C:\\Users\\studwilksa2535\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\attr_value_pb2.py:40: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _descriptor.FieldDescriptor(\n",
      "C:\\Users\\studwilksa2535\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\core\\framework\\attr_value_pb2.py:33: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _ATTRVALUE_LISTVALUE = _descriptor.Descriptor(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:46: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gym\n",
    "import gym_donkeycar\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 as cv\n",
    "import time\n",
    "import pickle\n",
    "import birds_eye_vector_space\n",
    "import random\n",
    "from pandas import Series, DataFrame\n",
    "from collections import deque\n",
    "\n",
    "#from keras.layers import Dense\n",
    "#from tensorflow.keras.optimizers import Adam\n",
    "#from keras.initializers import normal, identity\n",
    "#from keras.models import model_from_json\n",
    "#from keras.models import Sequential\n",
    "#from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "#from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.python.client import device_lib\n",
    "#from keras import backend as K\n",
    "#import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use gpu\n",
    "#K.tensorflow_backend._get_available_gpus()\n",
    "#sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "#K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  2\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_built_with_cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\studwilksa2535\\AppData\\Local\\Temp\\ipykernel_19864\\565982385.py:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available(cuda_only=False, min_cuda_compute_capability=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run on the second GPU\n",
    "#os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\";\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 5409559662098405757\n",
       " xla_global_id: -1,\n",
       " name: \"/device:GPU:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 9401860096\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 16717800033607018441\n",
       " physical_device_desc: \"device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:49:00.0, compute capability: 7.5\"\n",
       " xla_global_id: 416903419,\n",
       " name: \"/device:GPU:1\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 6271991808\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 6882144710146823094\n",
       " physical_device_desc: \"device: 1, name: GeForce RTX 2070 SUPER, pci bus id: 0000:21:00.0, compute capability: 7.5\"\n",
       " xla_global_id: 2144165316]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tf.config.list_physical_devices('GPU'))\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mirrored_strategy = tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supress scientific notation like: e*+03\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANCHOR_POINTS = 8\n",
    "ROAD_ROI = np.array([(120,90),(200,90),(0,200),(320,200)],dtype='float32')\n",
    "WARPED_IMAGE_SHAPE = np.array([[10,320],[0,0],[200,0],[200,310]],np.int32)            # NEW IMAGE Shape after Warping !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "WARPED_IMAGE_HEIGHT = 320\n",
    "WARPED_IMAGE_WIDTH = 200\n",
    "NUMBER_OF_DEPTH_LAYERS = 50\n",
    "\n",
    "VECTOR_SPACE_IMAGE_ROWS = 50\n",
    "VECTOR_SPACE_IMAGE_COLUMNS = 100\n",
    "VECTOR_SPACE_IMAGE_CHANNELS = 4 # 4*(50,100) stacked frames "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to save the plots\n",
    "def save_plot(data, name, episode):\n",
    "    plt.figure(figsize=(8,5), frameon=True)\n",
    "    #plt.plot([episode for episode in range(len(data))], data)\n",
    "    plt.plot([ep for ep in range(episode)], data)\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel(name)\n",
    "    figplot = '%s-episode-%s.png' % (name,episode)\n",
    "    #figplot = '{}-episode-{}.png'.format(name,episode)\n",
    "    #location = '{}/saves/torch/{}'.format(os.getcwd(),figplot)\n",
    "    #location = '%s/collected_data/plots/%s' % (os.getcwd(),figplot)\n",
    "    location = 'C:\\\\Users\\\\studwilksa2535\\\\Desktop\\\\DonkeyCarAI\\\\collected_data\\\\plots\\\\%s' % figplot \n",
    "    plt.savefig(location, transparent=False)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data_as_dataframe(episode_number, reward, loss, measured_time, max_cte, average_cte, last_lap_time):\n",
    "    episodes = [i for i in range(1, episode_number+1)]\n",
    "    collected_data = {'episode': episodes, 'reward': reward, 'loss':loss, 'time': measured_time, 'max_cte': max_cte, 'average_cte': average_cte, 'lap_time': last_lap_time}\n",
    "    df_data = DataFrame.from_dict(collected_data).set_index('episode')\n",
    "    \n",
    "    df_name = 'data-episode-%s.pkl' % episode\n",
    "    #location = '%s/collected_data/raw_data/%s' % (os.getcwd(),df_name)\n",
    "    location = 'C:\\\\Users\\\\studwilksa2535\\\\Desktop\\\\DonkeyCarAI\\\\collected_data\\\\raw_data\\\\%s' % df_name\n",
    "    df_data.to_pickle(location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector space is initialized\n",
      "loading camera parameters\n"
     ]
    }
   ],
   "source": [
    "vector_space = birds_eye_vector_space.Vector_space(ANCHOR_POINTS, ROAD_ROI, WARPED_IMAGE_SHAPE, WARPED_IMAGE_HEIGHT, WARPED_IMAGE_WIDTH, NUMBER_OF_DEPTH_LAYERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\studwilksa2535\\\\Desktop\\\\DonkeyCarAI\\\\Master-Thesis-Development-of-a-Deep-RL-Model-for-simulated-Driving-2D-Vector-Space\\\\docker\\\\src'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    \n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.t = 0\n",
    "        self.max_Q = 0\n",
    "        self.train = True\n",
    "        # Set to True to train on images with segmented lane lines\n",
    "        self.lane_detection = False\n",
    "        \n",
    "        # Huber loss\n",
    "        #self.huber_loss = tf.keras.losses.Huber(reduction=tf.keras.losses.Reduction.SUM)\n",
    "        \n",
    "        # Get size of state and action\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        # CNN Input\n",
    "        #self.input_shape = (VECTOR_SPACE_IMAGE_CHANNELS, VECTOR_SPACE_IMAGE_ROWS, VECTOR_SPACE_IMAGE_COLUMNS, 4)   # == 4 * (50, 100, 1); keras: (1,50,100,4)\n",
    "        self.input_shape = (1, VECTOR_SPACE_IMAGE_ROWS, VECTOR_SPACE_IMAGE_COLUMNS, VECTOR_SPACE_IMAGE_CHANNELS)   # == 4 * (50, 100, 1); keras: (1,50,100,4)\n",
    "        \n",
    "        # These are hyper parameters for the DQN\n",
    "        self.discount_factor = 0.99\n",
    "        self.learning_rate = 1e-4\n",
    "        if (self.train):\n",
    "            self.epsilon = 1.0\n",
    "            self.initial_epsilon = 1.0\n",
    "        else:\n",
    "            self.epsilon = 1e-6\n",
    "            self.initial_epsilon = 1e-6\n",
    "        self.epsilon_min = 0.02\n",
    "        self.batch_size = 512\n",
    "        self.train_start = 100\n",
    "        self.explore = 10000\n",
    "        \n",
    "        # Create replay memory using deque\n",
    "        self.memory = deque(maxlen=10000)\n",
    "        \n",
    "        # Create main model and target model                # Double DQN !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "        self.model = self.build_model()\n",
    "        self.target_model = self.build_model()\n",
    "\n",
    "        # Copy the model to target model\n",
    "        # --> initialize the target model so that the parameters of model & target model to be same\n",
    "        self.update_target_model()\n",
    "        \n",
    "    def build_model_old(self):\n",
    "        print('delte this')\n",
    "        #model = Sequential()\n",
    "        #model.add(Convolution2D(32, 8, 8, subsample=(4, 4), border_mode='same',input_shape=(VECTOR_SPACE_IMAGE_ROWS,VECTOR_SPACE_IMAGE_COLUMNS,VECTOR_SPACE_IMAGE_CHANNELS)))  #80*80*4\n",
    "        #model.add(Activation('relu'))\n",
    "        #model.add(Convolution2D(64, 4, 4, subsample=(2, 2), border_mode='same'))\n",
    "        #model.add(Activation('relu'))\n",
    "        #model.add(Convolution2D(64, 3, 3, subsample=(1, 1), border_mode='same'))\n",
    "        #model.add(Activation('relu'))\n",
    "        #model.add(Flatten())\n",
    "        #model.add(Dense(512))\n",
    "        #model.add(Activation('relu'))\n",
    "\n",
    "        ## 15 categorical bins for Steering angles\n",
    "        #model.add(Dense(15, activation=\"linear\")) \n",
    "\n",
    "        #adam = Adam(lr=self.learning_rate)\n",
    "        #model.compile(loss='mse',optimizer=adam)\n",
    "\n",
    "        #return model\n",
    "\n",
    "    def build_model(self):\n",
    "        model = keras.Sequential([\n",
    "            layers.Conv2D(filters=32, strides=(4, 4),kernel_size=8, padding='same', activation='relu', input_shape=(self.input_shape[1:])),\n",
    "            layers.Conv2D(filters=64, strides=(2, 2),kernel_size=4, padding='same', activation='relu'),\n",
    "            layers.Conv2D(filters=64, strides=(1, 1),kernel_size=3, padding='same', activation='relu'),\n",
    "            layers.Flatten(),\n",
    "            layers.Dense(units=512,activation='relu'),\n",
    "            layers.Dense(units=512,activation='relu'), # TODO ADDED THIS LAYER FOR TESTING\n",
    "            layers.Dense(units=15,activation='linear'), \n",
    "        ])\n",
    "        #optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
    "        optimizer = tf.keras.optimizers.Adam()                                  # NORMAL: TODO UNCOMMENT THIS\n",
    "        #optimizer = tf.keras.optimizers.SGD()\n",
    "        model.compile(loss='mse', optimizer=optimizer)                          # NORMAL: TODO UNCOMMENT THIS\n",
    "        #model.compile(loss=tf.keras.losses.CategoricalCrossentropy(), optimizer=optimizer)\n",
    "        return model\n",
    "    \n",
    "    def calculate_loss(self, y_true, y_pred):    \n",
    "        y_pred = tf.convert_to_tensor_v2(y_pred)\n",
    "        y_true = tf.cast(y_true, y_pred.dtype)\n",
    "        return tf.reduce_mean(math_ops.square(y_pred - y_true), axis=-1)\n",
    "    \n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    # Get action from model using epsilon-greedy policy\n",
    "    def get_action(self, s_t):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            #print(\"Return Random Value\")\n",
    "            #return random.randrange(self.action_size)\n",
    "            return np.random.uniform(-1,1)\n",
    "        else:\n",
    "            #print(\"Return Max Q Prediction\")\n",
    "            q_value = self.model.predict(s_t)\n",
    "            # Convert q array to steering value\n",
    "            return linear_unbin(q_value[0])\n",
    "\n",
    "    def replay_memory(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        #if self.epsilon > self.epsilon_min:\n",
    "        #    #self.epsilon *= self.epsilon_decay\n",
    "        #    self.epsilon -= (self.initial_epsilon - self.epsilon_min) / self.explore\n",
    "\n",
    "    def train_replay(self):\n",
    "        if len(self.memory) < self.train_start:\n",
    "            return\n",
    "        \n",
    "        batch_size = min(self.batch_size, len(self.memory))\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "\n",
    "        state_t, action_t, reward_t, state_t1, terminal = zip(*minibatch)\n",
    "        #print('Starting training\\nstate_t: %s\\naction_t: %s\\nreward_t: %s\\nstate_1: %s\\nterminal: %s' % (len(state_t), len(action_t), len(reward_t), len(state_t1), len(terminal)))\n",
    "        state_t = np.concatenate(state_t)\n",
    "        state_t1 = np.concatenate(state_t1)\n",
    "        #print('Concat:\\nstate: %s\\nstate_t1: %s' % (state_t, state_t1))\n",
    "        targets = self.model.predict(state_t)\n",
    "        self.max_Q = np.max(targets[0])\n",
    "        target_val = self.model.predict(state_t1)\n",
    "        target_val_ = self.target_model.predict(state_t1)\n",
    "        for i in range(batch_size):\n",
    "            if terminal[i]:\n",
    "                targets[i][action_t[i]] = reward_t[i]\n",
    "            else:\n",
    "                a = np.argmax(target_val[i])\n",
    "                targets[i][action_t[i]] = reward_t[i] + self.discount_factor * (target_val_[i][a])\n",
    "\n",
    "        \n",
    "        # train on the current batch and return a dictionary with the loss and so on\n",
    "        #metrics = self.model.train_on_batch(x=state_t, y=targets, reset_metrics=True, return_dict=True)           \n",
    "        metrics = self.model.train_on_batch(x=state_t, y=targets, reset_metrics=True, return_dict=True) #reset Metrix different\n",
    "        return metrics['loss']\n",
    "        \n",
    "    def load_model(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    # Save the model which is under training\n",
    "    def save_model(self, name):\n",
    "        self.model.save_weights(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_bin(a):\n",
    "    \"\"\"\n",
    "    Convert a value to a categorical array.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    a : int or float\n",
    "        A value between -1 and 1\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list of int\n",
    "        A list of length 15 with one item set to 1, which represents the linear value, and all other items set to 0.\n",
    "    \"\"\"\n",
    "    a = a + 1\n",
    "    b = round(a / (2 / 14))\n",
    "    arr = np.zeros(15)\n",
    "    arr[int(b)] = 1\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_unbin(arr):\n",
    "    \"\"\"\n",
    "    Convert a categorical array to value.\n",
    "\n",
    "    See Also\n",
    "    --------\n",
    "    linear_bin\n",
    "    \"\"\"\n",
    "    if not len(arr) == 15:\n",
    "        raise ValueError('Illegal array length, must be 15')\n",
    "    b = np.argmax(arr)\n",
    "    a = b * (2 / 14) - 1\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gym_donkeycar.core.client:connecting to localhost:9091 \n",
      "C:\\Users\\studwilksa2535\\AppData\\Roaming\\Python\\Python38\\site-packages\\gym\\spaces\\box.py:84: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n",
      "WARNING:gym_donkeycar.envs.donkey_sim:waiting for sim to start..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting DonkeyGym env\n",
      "Setting default: start_delay 5.0\n",
      "Setting default: max_cte 5.0\n",
      "Setting default: frame_skip 1\n",
      "Setting default: cam_resolution (120, 160, 3)\n",
      "Setting default: log_level 20\n",
      "Setting default: host localhost\n",
      "Setting default: port 9091\n",
      "loading scene generated_road\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gym_donkeycar.envs.donkey_sim:on need car config\n",
      "INFO:gym_donkeycar.envs.donkey_sim:sending car config.\n",
      "INFO:gym_donkeycar.envs.donkey_sim:done sending cam config. {'img_w': '320', 'img_h': '200', 'img_d': '1', 'img_enc': 'PNG', 'fov': '90', 'fish_eye_x': '0.0', 'fish_eye_y': '0.0', 'offset_x': '0.0', 'offset_y': '0.0', 'offset_z': '0.0', 'rot_x': '0'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REWARD: 18.42068214629976\tCTE: 0.000169716\n",
      "REWARD: 18.502211641502395\tCTE: 3.571933e-05\n",
      "REWARD: 18.57796563241857\tCTE: 0.0006837463\n",
      "REWARD: 18.64357222440499\tCTE: 0.001451396\n",
      "REWARD: 18.712839615584947\tCTE: 0.002243121\n",
      "REWARD: 18.77688306758802\tCTE: 0.001976695\n",
      "REWARD: 18.841905109067497\tCTE: 0.003110833\n",
      "REWARD: 18.907013789636018\tCTE: 0.00299568\n",
      "REWARD: 18.975267410760853\tCTE: 0.004431203\n",
      "REWARD: 19.046614892295565\tCTE: 0.004494446\n",
      "REWARD: 19.1080137065242\tCTE: 0.002577751\n",
      "REWARD: 19.163875973973933\tCTE: 0.000590421\n",
      "REWARD: 19.21980376694151\tCTE: 6.304964e-05\n",
      "REWARD: 19.26742836850263\tCTE: 0.0002956754\n",
      "REWARD: 19.31042689971342\tCTE: 0.002313321\n",
      "REWARD: 19.373331525561515\tCTE: 0.002405382\n",
      "REWARD: 19.42412636681807\tCTE: 0.004959854\n",
      "REWARD: 19.486716352391326\tCTE: 0.01124721\n",
      "REWARD: 19.51935967144833\tCTE: 0.01405491\n",
      "REWARD: 19.54807793388561\tCTE: 0.01159632\n",
      "REWARD: 19.608725768987323\tCTE: 0.0158412\n",
      "REWARD: 19.67211134334288\tCTE: 0.01915394\n",
      "REWARD: 19.763439386729658\tCTE: 0.02437257\n",
      "REWARD: 19.805732445350664\tCTE: 0.0226445\n",
      "REWARD: 19.827013840352244\tCTE: 0.02264022\n",
      "REWARD: 19.85186707679568\tCTE: 0.03035798\n",
      "REWARD: 19.877699923095253\tCTE: 0.02895865\n",
      "REWARD: 19.92402929759644\tCTE: 0.01169457\n",
      "REWARD: 19.970287683283715\tCTE: 0.02376054\n",
      "REWARD: 20.002177450155244\tCTE: 0.03598111\n",
      "REWARD: 20.03029862683212\tCTE: 0.03697902\n",
      "REWARD: 20.08745966328471\tCTE: 0.03182818\n",
      "REWARD: 20.150883997682534\tCTE: 0.02651089\n",
      "REWARD: 20.189341602993284\tCTE: 0.02359012\n",
      "REWARD: 20.22447880328724\tCTE: 0.03137126\n",
      "REWARD: 20.276713332578602\tCTE: 0.0424176\n",
      "REWARD: 20.32154060155517\tCTE: 0.04977193\n",
      "REWARD: 20.352330811166333\tCTE: 0.05099255\n",
      "REWARD: 20.360706968720713\tCTE: 0.05933751\n",
      "REWARD: 20.37486343890272\tCTE: 0.05713685\n",
      "REWARD: 20.420453088809644\tCTE: 0.0580392\n",
      "REWARD: 20.461636912233185\tCTE: 0.05932049\n",
      "REWARD: 20.51713705153597\tCTE: 0.05652793\n",
      "REWARD: 20.55621050982792\tCTE: 0.05371751\n",
      "REWARD: 20.590407575909023\tCTE: 0.06057402\n",
      "REWARD: 20.631807390800684\tCTE: 0.0729552\n",
      "REWARD: 20.67082317461363\tCTE: 0.08672274\n",
      "REWARD: 20.70328911429886\tCTE: 0.1063425\n",
      "REWARD: 20.694476870555242\tCTE: 0.08850528\n",
      "REWARD: 20.672196122270442\tCTE: 0.1041412\n",
      "REWARD: 20.709743888501645\tCTE: 0.1093494\n",
      "REWARD: 20.746934139814044\tCTE: 0.1089591\n",
      "REWARD: 20.774560021310222\tCTE: 0.1148337\n",
      "REWARD: 20.785439429625164\tCTE: 0.114789\n",
      "REWARD: 20.794825327020305\tCTE: 0.1260987\n",
      "REWARD: 20.830580243587523\tCTE: 0.1441003\n",
      "REWARD: 20.858625918695445\tCTE: 0.1628571\n",
      "REWARD: 20.861270533504822\tCTE: 0.1696913\n",
      "REWARD: 20.905481656764564\tCTE: 0.1670395\n",
      "REWARD: 20.947827726971965\tCTE: 0.1685185\n",
      "REWARD: 20.977300349533163\tCTE: 0.1669976\n",
      "REWARD: 21.026693870254242\tCTE: 0.1616241\n",
      "REWARD: 21.051531554950643\tCTE: 0.1634202\n",
      "REWARD: 21.092508246493125\tCTE: 0.1719673\n",
      "REWARD: 21.141561481266525\tCTE: 0.1531039\n",
      "REWARD: 21.178088085191483\tCTE: 0.1734478\n",
      "REWARD: 21.191894153682263\tCTE: 0.1895855\n",
      "REWARD: 21.175578695848284\tCTE: 0.1963854\n",
      "REWARD: 21.181473155370963\tCTE: 0.2120415\n",
      "REWARD: 21.209298890789963\tCTE: 0.2257145\n",
      "REWARD: 21.200122403688503\tCTE: 0.2488193\n",
      "REWARD: 3.3553378093168\tCTE: 0.2810076\n",
      "REWARD: 3.412962241098\tCTE: 0.307863\n",
      "REWARD: 3.4776581596688\tCTE: 0.343627\n",
      "REWARD: 3.5667145984970805\tCTE: 0.3845263\n",
      "REWARD: 3.63945467394256\tCTE: 0.4170004\n",
      "REWARD: 3.71258515384672\tCTE: 0.4511646\n",
      "REWARD: 3.7815503468878\tCTE: 0.4784711\n",
      "REWARD: 3.7601166929547998\tCTE: 0.4824335\n",
      "REWARD: 3.4058190763104546\tCTE: 0.5331392\n",
      "REWARD: 3.292578991158979\tCTE: 0.5701744\n",
      "REWARD: 3.2513838182586405\tCTE: 0.5953442\n",
      "REWARD: 3.2012615948523795\tCTE: 0.6288912\n",
      "REWARD: 3.1568198574374913\tCTE: 0.6675173\n",
      "REWARD: 3.0941545792576326\tCTE: 0.7074639\n",
      "REWARD: 2.988178797553129\tCTE: 0.7576172\n",
      "REWARD: 2.854154985874288\tCTE: 0.7937843\n",
      "REWARD: 2.7990161486070573\tCTE: 0.8339422\n",
      "REWARD: 2.762744068441685\tCTE: 0.8733485\n",
      "REWARD: 2.70193037444984\tCTE: 0.9060582\n",
      "REWARD: 2.69728686561965\tCTE: 0.9299484\n",
      "REWARD: 2.7688142054135745\tCTE: 0.9099635\n",
      "REWARD: 2.7431935799567735\tCTE: 0.9450344\n",
      "REWARD: 2.714935410503073\tCTE: 0.9823896\n",
      "REWARD: 0.6428243285196\tCTE: 1.023906\n",
      "REWARD: 0.5407487501128005\tCTE: 1.073338\n",
      "REWARD: 0.38414747997440024\tCTE: 1.110752\n",
      "REWARD: 0.34938838519500015\tCTE: 1.136421\n",
      "REWARD: 0.31744159856480003\tCTE: 1.161818\n",
      "REWARD: 0.27080515599840016\tCTE: 1.185228\n",
      "REWARD: 0.25539049055920016\tCTE: 1.200546\n",
      "REWARD: 0.24310766651319993\tCTE: 1.211718\n",
      "REWARD: 0.17070201981879984\tCTE: 1.231533\n",
      "REWARD: 0.15985046685879967\tCTE: 1.245006\n",
      "REWARD: 0.22968688283119976\tCTE: 1.229276\n",
      "REWARD: 0.21525842736520007\tCTE: 1.245579\n",
      "REWARD: 0.19155070286719988\tCTE: 1.264964\n",
      "REWARD: 0.1514388375960003\tCTE: 1.28206\n",
      "REWARD: 0.1537170599059996\tCTE: 1.290221\n",
      "REWARD: 0.15826721273659983\tCTE: 1.294267\n",
      "REWARD: 0.1158646283828002\tCTE: 1.309481\n",
      "REWARD: 0.10466496865840025\tCTE: 1.319316\n",
      "REWARD: 0.09306239825839979\tCTE: 1.328673\n",
      "REWARD: 0.09352243139520011\tCTE: 1.327024\n",
      "REWARD: 0.0793293744086001\tCTE: 1.333213\n",
      "REWARD: 0.07724051004620014\tCTE: 1.331029\n",
      "REWARD: -0.005817006050000018\tCTE: 1.34365\n",
      "REWARD: 0.012572890951399796\tCTE: 1.347971\n",
      "REWARD: 0.016138106928799356\tCTE: 1.351282\n",
      "REWARD: -0.04653978532799963\tCTE: 1.368294\n",
      "REWARD: -0.09684551299099997\tCTE: 1.375807\n",
      "REWARD: -0.11596185051559971\tCTE: 1.378426\n",
      "REWARD: -0.1310338667737998\tCTE: 1.393173\n",
      "REWARD: -0.14266161058539994\tCTE: 1.407537\n",
      "REWARD: -0.1484192517320002\tCTE: 1.41859\n",
      "REWARD: -0.1393186137565996\tCTE: 1.423347\n",
      "REWARD: -0.03747044432879987\tCTE: 1.394674\n",
      "REWARD: -0.07389530655400023\tCTE: 1.409942\n",
      "REWARD: -0.1702954581188001\tCTE: 1.439802\n",
      "REWARD: -0.24958300346159978\tCTE: 1.456788\n",
      "REWARD: -0.2693288442148001\tCTE: 1.467659\n",
      "REWARD: -0.2960207171224001\tCTE: 1.485847\n",
      "REWARD: -0.3122875307044004\tCTE: 1.499702\n",
      "REWARD: -0.3303549043521996\tCTE: 1.513941\n",
      "REWARD: -0.3899309470702006\tCTE: 1.524647\n",
      "REWARD: -0.36337391890119974\tCTE: 1.520006\n",
      "REWARD: -0.36608977727519987\tCTE: 1.521708\n",
      "REWARD: -0.3079667888333999\tCTE: 1.510353\n",
      "REWARD: -0.3633218486469998\tCTE: 1.528873\n",
      "REWARD: -0.39498772964439954\tCTE: 1.549721\n",
      "REWARD: -0.46477263151360004\tCTE: 1.556908\n",
      "REWARD: -0.45775022281679956\tCTE: 1.561794\n",
      "REWARD: -0.5356473201299998\tCTE: 1.581594\n",
      "REWARD: -0.5798601332458002\tCTE: 1.597599\n",
      "REWARD: -0.6004898220376003\tCTE: 1.605668\n",
      "REWARD: -0.6366278384851998\tCTE: 1.624914\n",
      "REWARD: -0.6821554240788004\tCTE: 1.648389\n",
      "REWARD: -0.7011868086778001\tCTE: 1.657609\n",
      "REWARD: -0.8029925307100001\tCTE: 1.697495\n",
      "REWARD: -0.8688562142707998\tCTE: 1.728553\n",
      "REWARD: -0.9191641387491996\tCTE: 1.754573\n",
      "REWARD: -0.9895812322989999\tCTE: 1.785223\n",
      "REWARD: -1.0778690955703998\tCTE: 1.822803\n",
      "REWARD: -1.2170126082088002\tCTE: 1.865813\n",
      "REWARD: -1.316530690384\tCTE: 1.89112\n",
      "REWARD: -1.430543200186\tCTE: 1.926555\n",
      "REWARD: -1.5261461172615998\tCTE: 1.958858\n",
      "REWARD: -1.5734698360180004\tCTE: 1.978765\n",
      "REWARD: -1.5090701432606002\tCTE: 1.954923\n",
      "REWARD: -1.5867780006703995\tCTE: 1.982074\n",
      "REWARD: -3.6920344535363996\tCTE: 2.016614\n",
      "REWARD: -3.8421829643820007\tCTE: 2.060195\n",
      "REWARD: -4.033547027028\tCTE: 2.111996\n",
      "REWARD: -4.191178981801201\tCTE: 2.153141\n",
      "REWARD: -4.3379789432022\tCTE: 2.195309\n",
      "REWARD: -4.4610877174236006\tCTE: 2.230322\n",
      "REWARD: -4.549640966144\tCTE: 2.25568\n",
      "REWARD: -4.602402527980001\tCTE: 2.27305\n",
      "REWARD: -4.585996415211599\tCTE: 2.272734\n",
      "REWARD: -4.707253883471999\tCTE: 2.309252\n",
      "REWARD: -4.839669798482401\tCTE: 2.348481\n",
      "REWARD: -4.97275434238\tCTE: 2.385412\n",
      "REWARD: -5.1494792567188\tCTE: 2.432583\n",
      "REWARD: -5.335541831997601\tCTE: 2.485733\n",
      "REWARD: -5.528939403331799\tCTE: 2.540591\n",
      "REWARD: -5.726291341967999\tCTE: 2.596144\n",
      "REWARD: -5.919173787571999\tCTE: 2.642331\n",
      "REWARD: -6.0314776137576\tCTE: 2.673652\n",
      "REWARD: -6.1531859930292\tCTE: 2.707906\n",
      "REWARD: -6.257621436378\tCTE: 2.731469\n",
      "REWARD: -6.474198928046399\tCTE: 2.790931\n",
      "REWARD: -6.6928134790836\tCTE: 2.850878\n",
      "REWARD: -6.905757146124799\tCTE: 2.908896\n",
      "REWARD: -7.122665110777802\tCTE: 2.967989\n",
      "REWARD: -10.4358233151204\tCTE: 3.037903\n",
      "REWARD: -10.7650038636272\tCTE: 3.107048\n",
      "REWARD: -11.062828450030398\tCTE: 3.168134\n",
      "REWARD: -11.411585843964001\tCTE: 3.23923\n",
      "REWARD: -11.784822243242\tCTE: 3.315905\n",
      "REWARD: -12.0006809045664\tCTE: 3.360252\n",
      "REWARD: -12.2861439832322\tCTE: 3.418619\n",
      "REWARD: -12.4977496028126\tCTE: 3.462419\n",
      "REWARD: -12.6644664095708\tCTE: 3.497302\n",
      "REWARD: -12.932322976639998\tCTE: 3.546248\n",
      "REWARD: -13.2169377636116\tCTE: 3.606227\n",
      "REWARD: -13.5599584431152\tCTE: 3.677291\n",
      "REWARD: -13.9340300142652\tCTE: 3.754858\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REWARD: -14.2540439897922\tCTE: 3.819987\n",
      "REWARD: -14.5595258707436\tCTE: 3.883547\n",
      "REWARD: -14.839641112144399\tCTE: 3.941039\n",
      "REWARD: -19.242596975056003\tCTE: 4.020524\n",
      "REWARD: -19.724807846160996\tCTE: 4.102765\n",
      "REWARD: -20.1182218372542\tCTE: 4.168101\n",
      "REWARD: -20.4353307950944\tCTE: 4.222804\n",
      "REWARD: -20.789650394825\tCTE: 4.283677\n",
      "REWARD: -21.202520459804802\tCTE: 4.354013\n",
      "REWARD: -21.699217148725005\tCTE: 4.438075\n",
      "REWARD: -22.121121799767998\tCTE: 4.508221\n",
      "REWARD: -22.496755924914396\tCTE: 4.572804\n",
      "REWARD: -22.632489066176802\tCTE: 4.595783\n",
      "REWARD: -22.9294777679998\tCTE: 4.646597\n",
      "REWARD: -23.28793596184\tCTE: 4.707803\n",
      "REWARD: -23.666747160160803\tCTE: 4.772722\n",
      "REWARD: -24.0554502034338\tCTE: 4.839103\n",
      "REWARD: -24.3935206054152\tCTE: 4.896732\n",
      "REWARD: -24.7369392321696\tCTE: 4.955248\n",
      "REWARD: -126.06761\tCTE: 5.013522\n",
      "EPISODE: 1 | TIME: 26.528529405593872 s | REWARD: 3.1063416060158695 | FRAMES: 217 | QMAX: nan | EPSILON: 1.0 | CTE: -1.4217325197459447 | LOSS: nan\n",
      "EPISODE: 2 | TIME: 12.093906879425049 s | REWARD: 12.620813390570493 | FRAMES: 77 | QMAX: nan | EPSILON: 0.99975 | CTE: -0.6867076736038962 | LOSS: nan\n",
      "EPISODE: 3 | TIME: 17.536239624023438 s | REWARD: 7.166219229562514 | FRAMES: 197 | QMAX: nan | EPSILON: 0.9995 | CTE: -1.1579384602812182 | LOSS: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#%% SET UP ENVIRONMENT\n",
    "# get same weights at the model weihgts init\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Normal one \n",
    "#os.environ['DONKEY_SIM_PATH'] = \"/home/zamy/masterthesis/DonkeySimLinux/donkey_sim.x86_64\"\n",
    "os.environ['DONKEY_SIM_PATH'] = \"C:\\\\Users\\\\studwilksa2535\\\\Desktop\\\\DonkeyCarAI\\\\test\\\\DonkeySimWin\\\\donkey_sim.exe\"\n",
    "os.environ['DONKEY_SIM_PORT'] = str(9091)\n",
    "os.environ['DONKEY_SIM_HEADLESS'] = str(0) # \"1\" is headless\n",
    "\n",
    "CAMERA_CONF = {'cam_config':{'img_w': '320',\n",
    "                        'img_h': '200',\n",
    "                        'img_d': '1',   # 3 for colored Tensor image\n",
    "                        'img_enc': 'PNG', \n",
    "                        'fov': '90', \n",
    "                        'fish_eye_x': '0.0', \n",
    "                        'fish_eye_y': '0.0', \n",
    "                        'offset_x': '0.0', \n",
    "                        'offset_y': '0.0', \n",
    "                        'offset_z': '0.0', \n",
    "                        'rot_x': '0'}}\n",
    "# Other tracks\n",
    "#env = gym.make(\"donkey-generated-track-v0\",conf=CAMERA_CONF)\n",
    "\n",
    "# TRACKS TRACKS - TKarmer Tracks\n",
    "#env = gym.make(\"donkey-generated-track-v0\",conf=CAMERA_CONF) #,conf=config\n",
    "# Random track, but the reward is kinda not working, IF THE REWARD WORKS IT IS KINDA RANDOM !!, this maybe debends on the environment\n",
    "env = gym.make(\"donkey-generated-roads-v0\",conf=CAMERA_CONF) #,conf=config\n",
    "\n",
    "# Mini Monaco Track\n",
    "#env = gym.make(\"donkey-minimonaco-track-v0\",conf=CAMERA_CONF) #,conf=config\n",
    "#env = gym.make(\"donkey-generated-track-v0\") #,conf=config\n",
    "#env = gym.wrappers.ResizeObservation(env,(200,320))\n",
    "\n",
    "## DELETE LATER\n",
    "#env.frameskip = 1\n",
    "#gym.wrappers.max\n",
    "\n",
    "# Create DQN Model\n",
    "# Get size of state and action from environment\n",
    "state_size = (VECTOR_SPACE_IMAGE_ROWS, VECTOR_SPACE_IMAGE_COLUMNS, VECTOR_SPACE_IMAGE_CHANNELS)\n",
    "action_size = 15 # Steering and Throttle\n",
    "throttle = 0.15 # Set the throttle as a constant value\n",
    "agent = DQNAgent(state_size, action_size)\n",
    "\n",
    "\n",
    "# Arrays for data collection / exploratory data analysis\n",
    "average_cte = []\n",
    "average_speed = []\n",
    "measured_time = []\n",
    "average_rewards = []\n",
    "average_loss = []\n",
    "collected_max_cte = []\n",
    "collected_lap_time = []\n",
    "\n",
    "# when a model and the data should be saved\n",
    "save_state = 200\n",
    "\n",
    "for episode in range(1,7001):\n",
    "    # Resetting the environment and preprocessing the first image\n",
    "    observation = env.reset()\n",
    "    # Graykonvertion of the observation image\n",
    "    observation = cv.cvtColor(observation, cv.COLOR_BGR2GRAY)\n",
    "    observation = vector_space.image_preprocessing(observation)\n",
    "    # frame stacking (4 times)\n",
    "    obv_stack = np.stack((observation, observation, observation, observation), axis= 2)\n",
    "    # reshaping for keras\n",
    "    obv_stack = obv_stack.reshape(1, obv_stack.shape[0], obv_stack.shape[1], obv_stack.shape[2])\n",
    "    \n",
    "    #action = np.array([0,0.10]) # drive straight with small speed\n",
    "    #action = np.array([1,0.1]) # drive straight with small speed\n",
    "    \n",
    "    # summed up values for data collection\n",
    "    # cumulative values for data collection\n",
    "    total_cte = float(0)\n",
    "    total_speed = float(0)\n",
    "    total_reward = float(0)\n",
    "    total_time = time.time()\n",
    "    total_loss = float(0)\n",
    "    max_cte = int(0)\n",
    "    lap_time = int(0)\n",
    "        \n",
    "    # counting the amount of frames per episode\n",
    "    frames = int(0)\n",
    "    \n",
    "    # boolean that describes if the env is done with this episode\n",
    "    done = False\n",
    "    while not done:\n",
    "        # incrementing the amount of frames per episode\n",
    "        frames += 1\n",
    "        \n",
    "        # Making a prediction for the current state\n",
    "        # and getting the information form the next step\n",
    "        steering = agent.get_action(obv_stack)\n",
    "        action = [steering, throttle]\n",
    "        next_observation, reward, done, info = env.step(action)\n",
    "        \n",
    "        current_cte = abs(info['cte'])\n",
    "        # punish and reward for a low or high 'CTE' with the logarithmic function\n",
    "        if current_cte < .25:\n",
    "            reward += abs(np.log(.10)) * 8  # 4 before\n",
    "        elif current_cte >= .25 and current_cte < .5:\n",
    "            reward += current_cte * 2\n",
    "        elif current_cte >= .5 and current_cte < 1: \n",
    "            reward += abs(np.log(current_cte))\n",
    "        elif current_cte >= 1 and current_cte < 2: \n",
    "            reward -= abs(current_cte) * 2\n",
    "        elif current_cte >= 2 and current_cte < 3: \n",
    "            reward -= abs(current_cte) * 3\n",
    "        elif current_cte >= 3 and current_cte < 4: \n",
    "            reward -= abs(current_cte) * 4\n",
    "        elif current_cte >=4:\n",
    "            reward -= abs(current_cte) * 5\n",
    "                    \n",
    "        # if the car leaves the track punish it\n",
    "        if done:\n",
    "            reward -= 100\n",
    "        # Check if the CTE of the environment is bugged\n",
    "        if episode == 1:\n",
    "            #print('CTE: %s\\tReward: %s' % (info['cte'], reward)\n",
    "            print('REWARD: %s\\tCTE: %s' % (reward, current_cte))\n",
    "        \n",
    "        # Graykonvertion of the observation image\n",
    "        next_observation = cv.cvtColor(next_observation, cv.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # preprocessing the new observation\n",
    "        next_observation_show = vector_space.image_preprocessing(next_observation)\n",
    "        # reshaping for keras\n",
    "        #next_observation = next_observation_show.reshape(1, next_observation_show.shape[0], next_observation_show.shape[1], 1)\n",
    "        # appending to the observation stack\n",
    "        #obv_stack = np.append(next_observation, obv_stack[:, :, :, :3], axis=3)\n",
    "        \n",
    "        obv_stack_t1 = np.stack([next_observation_show, obv_stack[0,:,:,0],obv_stack[0,:,:,1],obv_stack[0,:,:,2]], axis=2)\n",
    "        obv_stack_t1 = obv_stack_t1.reshape(1, obv_stack_t1.shape[0], obv_stack_t1.shape[1], obv_stack_t1.shape[2])\n",
    "                \n",
    "        # saving the sample <s, a, r, s'> to the replay memory\n",
    "        agent.replay_memory(obv_stack, np.argmax(linear_bin(steering)), reward, obv_stack_t1, done)        \n",
    "        \n",
    "        # training the DDQN, if training is enabled\n",
    "        #if agent.train:\n",
    "        #    loss = agent.train_replay()\n",
    "        #    if loss != None:\n",
    "        #        total_loss += loss\n",
    "        #        #print('loss: %s ' % (total_loss / frames))\n",
    "        \n",
    "        # overwriting the stack and incrementing the time/frame counter\n",
    "        obv_stack = obv_stack_t1\n",
    "        agent.t += 1       \n",
    "        \n",
    "        # adding up the collected data\n",
    "        current_cte = info['cte'] \n",
    "        total_cte += current_cte\n",
    "        total_speed += info['speed']\n",
    "        total_reward += reward\n",
    "        # update if there is any change in incrementation \n",
    "        if max_cte < current_cte:\n",
    "            max_cte = current_cte\n",
    "        if lap_time < info['last_lap_time']:\n",
    "            lap_time = info['last_lap_time']\n",
    "            \n",
    "        cv.imshow('vec img', next_observation_show)\n",
    "        \n",
    "        # stop the training if the car passes 120 seconds,\n",
    "        # so there is no infinit learning\n",
    "        if abs(total_time - time.time()) >= 120:\n",
    "            done = True\n",
    "\n",
    "        # Graykonvertion of the observation image\n",
    "        #observation = cv.cvtColor(observation, cv.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Image preprocessing\n",
    "        #vec_image = vector_space.image_preprocessing(observation)\n",
    "        \n",
    "        #print('REWARD: %s' % reward)\n",
    "        \n",
    "        #cv.imshow('vec img', vec_image)\n",
    "        \n",
    "        #break       # TODO: DELTE LATER\n",
    "        if cv.waitKey(25) & 0xFF == ord('q'):\n",
    "            cv.destroyAllWindows()\n",
    "            break\n",
    "    \n",
    "    # Training after each episode\n",
    "    # do 10 training sets of each with a batch size of 512\n",
    "    for i in range(0,10):\n",
    "        if agent.train:\n",
    "            total_loss += agent.train_replay()\n",
    "            #total_loss = agent.train_replay()\n",
    "            #if loss != None:\n",
    "            #    total_loss = loss\n",
    "    \n",
    "    \n",
    "    # AFTER this Episode & the environment returns True for the done variable\n",
    "    # updating the target DDQN\n",
    "    agent.update_target_model()\n",
    "    \n",
    "    # calculating the average loss in this episode\n",
    "    #episode_loss = total_loss.copy() #/ frames\n",
    "    total_loss = total_loss / 512 * 10\n",
    "    episode_cte = total_cte / frames\n",
    "    episode_reward = total_reward / frames\n",
    "    \n",
    "    # updating the time parameter\n",
    "    total_time = abs(total_time - time.time())\n",
    "    \n",
    "    # appending the collected data\n",
    "    average_cte.append(episode_cte)\n",
    "    average_speed.append(total_speed / frames)\n",
    "    measured_time.append(total_time)\n",
    "    average_rewards.append(episode_reward)\n",
    "    average_loss.append(total_loss)\n",
    "    collected_max_cte.append(max_cte)\n",
    "    collected_lap_time.append(lap_time) \n",
    "    \n",
    "    # Testing of saving the plot\n",
    "    #if episode % 3 == 0 and episode != 0:\n",
    "    #    save_plot(colleted_rewards, 'Reward', episode)\n",
    "    #    cv.destroyAllWindows()\n",
    "    #    break\n",
    "    \n",
    "    # Print episode information    \n",
    "    print('EPISODE: %s | TIME: %s s | REWARD: %s | FRAMES: %s | QMAX: %s | EPSILON: %s | CTE: %s | LOSS: %s' % (episode, total_time, episode_reward, frames, str(agent.max_Q), agent.epsilon, episode_cte, total_loss))    \n",
    "    \n",
    "    # update the epsilon after each episode\n",
    "    # Epsilion decay over time/amount of trainings\n",
    "    if agent.epsilon >= agent.epsilon_min:\n",
    "        #self.epsilon *= self.epsilon_decay\n",
    "        #self.epsilon -= (self.initial_epsilon - self.epsilon_min) / self.explore\n",
    "        #self.epsilon -= 0.00025\n",
    "        agent.epsilon -= 0.00025 # before 0.00035\n",
    "    \n",
    "    if episode % save_state == 0 and episode != 0:\n",
    "        print('saving after %s episodes' % save_state)\n",
    "        # saving the model\n",
    "        agent.save_model('C:\\\\Users\\\\studwilksa2535\\\\Desktop\\\\DonkeyCarAI\\\\models\\\\model_episode_%s.h5' % episode)        \n",
    "        \n",
    "        # Saving the data as plots\n",
    "        save_plot(average_rewards, 'Reward', episode)\n",
    "        save_plot(average_cte, 'Average cte', episode)\n",
    "        save_plot(collected_max_cte, 'Max cte', episode)\n",
    "        save_plot(measured_time, 'Time', episode)\n",
    "        save_plot(average_loss, 'Loss', episode)\n",
    "        save_plot(collected_lap_time, 'Lap Time', episode)\n",
    "        \n",
    "        # Saving the collected data\n",
    "        save_data_as_dataframe(episode, average_rewards, average_loss, measured_time, collected_max_cte, average_cte, collected_lap_time)\n",
    "    \n",
    "    # closing all cv windows\n",
    "    cv.destroyAllWindows()\n",
    "\n",
    "# closing all cv windows\n",
    "cv.destroyAllWindows()\n",
    "# Close the enviroment\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
